\documentclass[12pt]{article}
\usepackage{amsmath}
%\usepackage{fullpage}
\usepackage[top=1in, bottom=0.8in, left=0.4in, right=0.4in]{geometry}
\usepackage{multicol}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{graphicx,fancyhdr}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{siunitx}
\usepackage{dsfont}
\usetikzlibrary{arrows}

\let\oldref\ref
\renewcommand{\ref}[1]{(\oldref{#1})}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\setlength{\columnsep}{0.1pc}

\fancypagestyle{plain}{}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[RO,LE]{\sffamily\bfseries\large Stanford University}
\fancyhead[LO,RE]{\sffamily\bfseries\large CS224d}
\fancyfoot[LO,RE]{\sffamily\bfseries\large Alex Adamson}
\fancyfoot[RO,LE]{\sffamily\bfseries\thepage}
\renewcommand{\headrulewidth}{1pt}
\renewcommand{\footrulewidth}{1pt}

\title{Deep Recurrent Neural Network}
\author{Alex Adamson \\ \texttt{aadamson@stanford.edu}}
\date{\today}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\pd}[1]{\frac{\partial}{\partial #1}}
\newcommand{\pdeq}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\indicator}[1]{\mathds{1}[#1]}


\sisetup{mode = math}

\begin{document}

  \maketitle

  \vspace{-0.3in}
  \rule{\linewidth}{0.4pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \section{Network specification}

    We borrow our specification from Irsoy and Cardie, 2014.

    For $i > 1$, we have
    \begin{align}
      \overrightarrow{h}_t^{(i)} &= f(\underrightarrow{\overrightarrow{W}}^{(i)} \overrightarrow{h}_t^{(i-1)} + \underleftarrow{\overrightarrow{W}}^{(i)} \overleftarrow{h}_t^{(i-1)} + \overrightarrow{V}^{(i)} \overrightarrow{h}_{t-1}^{(i)} + \overrightarrow{b}^{(i)}) \\
      \overleftarrow{h}_t^{(i)} &= f(\underrightarrow{\overleftarrow{W}}^{(i)} \overrightarrow{h}_t^{(i-1)} + \underleftarrow{\overleftarrow{W}}^{(i)} \overleftarrow{h}_t^{(i-1)} + \overleftarrow{V}^{(i)} \overleftarrow{h}_{t=1}^{(i)} + \overleftarrow{b}^{(i)})
    \end{align}
    and for $i = 1$ we have
    \begin{align}
      \overrightarrow{h}_t^{(1)} &= f(\overrightarrow{W}^{(1)} x_t + \overrightarrow{V}^{(1)} \overrightarrow{h}_{t-1}^{(1)} + \overrightarrow{b}^{(1)}) \\
      \overleftarrow{h}_t^{(1)} &= f(\overleftarrow{W}^{(1)} x_t + \overleftarrow{V}^{(1)} \overleftarrow{h}_{t+1}^{(1)} + \overleftarrow{b}^{(1)})
    \end{align}

    We only connect the last layer (which we denote layer $L$) to the output layer:
    \begin{align}
      y_t &= g(\underrightarrow{U} \overrightarrow{h}_{t}^{(L)} + \underleftarrow{U} \overleftarrow{h}_{t}^{(L)} + c)
    \end{align}

  \section{Backpropagation derivation}

    For convenience, we use error vector notation when deriving the backpropagation updates.

    For our loss function, we choose categorical cross-entropy.

    Let $\delta_t^y$ be the error vector propagated by the softmax unit at timestep t.

    Let $\overrightarrow{\delta}_t^{(i)}$ be the error vector propagated by the 
    forward hidden unit in layer $i$ at timestep t.

    Let $\overleftarrow{\delta}_t^{(i)}$ be the error vector propagated by the 
    backward hidden unit in layer $i$ at timestep t.

    Let $f^*$ be the function such that $f^* (f(x)) = f'(x)$ where $f$ is as above.

    Then:
    \begin{align}
      \delta_t^y &= \hat{y}_t - y_t \\
      \overrightarrow{\delta}_t^{(i)} &= f^* (\overrightarrow{h}_t^{(i)}) \circ ((\underrightarrow{\overrightarrow{W}}^{(i+1)})^T \overrightarrow{\delta}_t^{(i+1)} + (\underrightarrow{\overleftarrow{W}}^{(i+1)})^T \overleftarrow{\delta}_t^{(i+1)} + (\overrightarrow{V}^{(i)})^T \overrightarrow{\delta}_{t+1}^{(i)}) \\
      \overleftarrow{\delta}_t^{(i)} &= f^* (\overleftarrow{h}_t^{(i)}) \circ ((\underleftarrow{\overrightarrow{W}}^{(i+1)})^T \overrightarrow{\delta}_t^{(i+1)} + (\underleftarrow{\overleftarrow{W}}^{(i+1)})^T \overleftarrow{\delta}_t^{(i+1)} + (\overleftarrow{V}^{(i)})^T \overleftarrow{\delta}_{t-1}^{(i)})
    \end{align}

    With these in hand, we can find the actual updates:
    \begin{align}
      \pdeq{J}{\overrightarrow{U}} &= \delta^y (\overrightarrow{h}^{(L)})^T \\
      \pdeq{J}{c} &= \delta^y \cdot \mathbf{1} \\
      \pdeq{J}{\underrightarrow{\overrightarrow{W}}^{(i)}} &= \overrightarrow{\delta}^{(i)} (\overrightarrow{h}^{(i-1)})^T \\
      \pdeq{J}{\underleftarrow{\overrightarrow{W}}^{(i)}} &= \overrightarrow{\delta}^{(i)} (\overleftarrow{h}^{(i-1)})^T \\
      \pdeq{J}{\overrightarrow{b}^{(i)}} &= \overrightarrow{\delta}^{(i)} \cdot \mathbf{1} \\
      \pdeq{J}{\underrightarrow{\overleftarrow{W}}^{(i)}} &= \overleftarrow{\delta}^{(i)} (\overrightarrow{h}^{(i-1)})^T \\
      \pdeq{J}{\underleftarrow{\overleftarrow{W}}^{(i)}} &= \overleftarrow{\delta}^{(i)} (\overleftarrow{h}^{(i-1)})^T \\
      \pdeq{J}{\overleftarrow{b}^{(i)}} &= \overleftarrow{\delta}^{(i)} \cdot \mathbf{1} \\
      \pdeq{J}{\overrightarrow{V}^{(i)}} &= \sum_{t=1}^T \overrightarrow{\delta}_t^{(i)} (\overrightarrow{h}_{t-1}^{(i)})^T \\
      \pdeq{J}{\overleftarrow{V}^{(i)}} &= \sum_{t=1}^T \overleftarrow{\delta}_t^{(i)} (\overleftarrow{h}_{t+1}^{(i)})^T \\
      \pdeq{J}{\overrightarrow{W}^{(1)}} &= \overrightarrow{\delta}^{(1)} x_t^T \\
      \pdeq{J}{\overleftarrow{W}^{(1)}} &= \overleftarrow{\delta}^{(1)} x_t^T
    \end{align}

\end{document}
